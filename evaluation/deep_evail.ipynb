{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evalutates our LLM by using the DeepEval library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class\n",
    "class FinanceLLM(DeepEvalBaseLLM):\n",
    "    # Constructor\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # Load the model\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    # Load the tokenizer\n",
    "    '''\n",
    "    @params: prompt - str\n",
    "    @returns: response - str\n",
    "    '''\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(**inputs, max_length=20)\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        return response\n",
    "\n",
    "    # Load the tokenizer\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    # Evaluate the model\n",
    "    def get_model_name(self):\n",
    "        return \"FinanceLLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Function to evaluate RAG\n",
    "def rag_evaluation():\n",
    "    # Test case\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What is the buy and hold strategy?\",\n",
    "        actual_output=\"The buy and hold strategy involves day trading and making quick profits.\",\n",
    "        expected_output=\"The buy and hold strategy involves purchasing stocks or other securities and holding them for a long period, regardless of market fluctuations.\",\n",
    "        retrieval_context=[\n",
    "            \"\"\"The buy and hold strategy is a long-term investment approach where investors purchase stocks or other securities and retain them for an extended period. This strategy is based on the belief that, despite volatility, the market will generally provide a good return over the long term.\"\"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Ishreet1/FinanceLLM\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Ishreet1/FinanceLLM\")\n",
    "    llm = FinanceLLM(model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluation retrivals\n",
    "    contextual_precision = ContextualPrecisionMetric(model=llm)\n",
    "    contextual_recall = ContextualRecallMetric(model=llm)\n",
    "    contextual_relevancy = ContextualRelevancyMetric(model=llm)\n",
    "\n",
    "    # Evaluation generation\n",
    "    answer_relevancy = AnswerRelevancyMetric(model=llm)\n",
    "    faithfulness = FaithfulnessMetric(model=llm)\n",
    "\n",
    "    # Evaluating RAG\n",
    "    print(evaluate(\n",
    "        test_cases=[test_case],\n",
    "        metrics=[\n",
    "            contextual_precision,\n",
    "            contextual_recall,\n",
    "            contextual_relevancy,\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "        ]\n",
    "    ))\n",
    "\n",
    "# Run the evaluation\n",
    "rag_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
